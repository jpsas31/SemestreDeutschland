{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using PyTorch you can either use your own Computer or [Google Colab](https://colab.research.google.com/).\n",
    "\n",
    "You need to install the [PyTorch](https://pytorch.org/) package which comes with some extra dependencies.\n",
    "\n",
    "Install the following packages for this notebook:\n",
    "- **PyTorch**\n",
    "- **torchvision**\n",
    "- **tqdm**\n",
    "- **matplotlib**\n",
    "\n",
    "If your computer is equpped with a GPU you can also install the GPU version of PyTorch. Otherwise install the CPU version, which is smaller in size and enough for the tasks of this practical.\n",
    "\n",
    "For using the GPU version you need to fullfill some prerequisites first, which are a little time consuming.\n",
    "- Make sure that your graphics card is new enough to handle the PyTorch environment. This can be checked by searching for the compute capability of your GPU and the compute capability requirements from the PyTorch module\n",
    "- Install the latest NVIDIA driver\n",
    "- Install suitable CUDA version\n",
    "- Install CudNN\n",
    "- Install PyTorch after all previous successful steps\n",
    "\n",
    "\n",
    "Using Google Colab should avoid installing the above mentioned prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 cpu torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# Tensors\n",
    "\n",
    "# Initialize a 1d torch tensor of size (6, 1) and name it 'data'. Initialize the tensor as random normal distribution\n",
    "\n",
    "# Code here\n",
    "\n",
    "data = torch.rand(size=(6,1))\n",
    "\n",
    "# Convert the torch tensor to a numpy array and convert it back afterwards. Keep the variable naming and just override the variable every time\n",
    "\n",
    "# Code here\n",
    "\n",
    "data = data.numpy()\n",
    "data = torch.from_numpy(data)\n",
    "\n",
    "\n",
    "# Tensors have a shape, a data type and are executed on some device on your computer. Find the mentioned tensor attributes and print them.\n",
    "\n",
    "# Code here\n",
    "\n",
    "print(data.dtype, data.device, data.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Slicing works the same as with numpy arrays. No need to learn a new syntax here :)\n",
    "# Try some slicing methods (i. e. the slicing methods we discussed in the first practical)\n",
    "\n",
    "# Code here\n",
    "slice1=data[0:5] #from 0 to 4\n",
    "slice2=data[-1] #last element\n",
    "slice3=data[1::2] #even rows\n",
    "\n",
    "\n",
    "###\n",
    "# Arithmetic operations\n",
    "###\n",
    "\n",
    "# Perform a matrix multiplication with two random tensors of different shape. The value initialization is of your choice.\n",
    "\n",
    "# Code here\n",
    "\n",
    "t1 = torch.rand(size=(6,4))\n",
    "t2 = torch.rand(size=(4,10))\n",
    "mult = torch.matmul(t1,t2)\n",
    "\n",
    "# Perform the hadamard (element-wise) product with two random initialized tensors.\n",
    "\n",
    "# Code here\n",
    "t1 = torch.rand(size=(6,4))\n",
    "t2 = torch.rand(size=(6,4))\n",
    "\n",
    "elementMult = torch.multiply(t1,t2)\n",
    "\n",
    "# For more useful tensor operations, plese check out their website: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Sequential and Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.state_dict of Linear(in_features=16, out_features=32, bias=True)>\n",
      "weights [Parameter containing:\n",
      "tensor([[ 0.1264,  0.1003,  0.0116,  0.1349, -0.0273,  0.2036,  0.0350,  0.1990,\n",
      "          0.0169, -0.2463, -0.1003, -0.1757, -0.1738, -0.0841,  0.0734,  0.2376],\n",
      "        [-0.1104,  0.0682, -0.2329,  0.0848, -0.1058,  0.0223,  0.0425,  0.2293,\n",
      "          0.0031, -0.1487, -0.0597,  0.2495, -0.0132, -0.1461,  0.1418, -0.1817],\n",
      "        [ 0.0160,  0.1380, -0.0544, -0.1887,  0.1822, -0.2147,  0.0158,  0.2230,\n",
      "          0.1254, -0.1107,  0.1657,  0.2018,  0.0319,  0.2444, -0.0551,  0.1927],\n",
      "        [-0.1379,  0.0799, -0.0337, -0.1541,  0.2478,  0.0783, -0.1238, -0.2299,\n",
      "         -0.1935,  0.0981,  0.0204, -0.0150, -0.0177,  0.0026, -0.1136,  0.2314],\n",
      "        [-0.1296,  0.1581,  0.1869,  0.1526,  0.0841, -0.0040, -0.0332,  0.0370,\n",
      "         -0.0205, -0.1895, -0.2208,  0.2036,  0.1255,  0.1535, -0.0496,  0.0987],\n",
      "        [-0.0629,  0.0135, -0.0394,  0.1137,  0.1325, -0.2108,  0.2277, -0.2273,\n",
      "          0.0312,  0.2325,  0.2343,  0.0050, -0.0098, -0.1615, -0.2016,  0.0307],\n",
      "        [ 0.0760,  0.1897, -0.0666, -0.0460, -0.1319,  0.1700,  0.0399, -0.0982,\n",
      "         -0.0461,  0.0892,  0.1661,  0.1667, -0.0633,  0.1942, -0.0502, -0.0501],\n",
      "        [-0.1215,  0.0976, -0.0579, -0.2132, -0.1594,  0.0351, -0.2251,  0.2390,\n",
      "          0.1318, -0.2345,  0.2320,  0.1566, -0.1846, -0.1129,  0.1688,  0.0908],\n",
      "        [-0.2289,  0.0171, -0.2333, -0.0201, -0.0923,  0.2317, -0.2040,  0.1970,\n",
      "          0.1727, -0.0878, -0.0099, -0.0583,  0.1268,  0.0763, -0.1742, -0.0274],\n",
      "        [-0.1106,  0.1733, -0.2064, -0.0052,  0.1267,  0.0397, -0.1344, -0.1872,\n",
      "         -0.2162,  0.0186, -0.1645, -0.2301, -0.0642,  0.0570, -0.1317,  0.1991],\n",
      "        [ 0.0683, -0.1555,  0.2143,  0.0977, -0.0405,  0.0083,  0.0922, -0.1922,\n",
      "         -0.0330, -0.0589, -0.2129, -0.0931,  0.0371, -0.0337, -0.1528, -0.0276],\n",
      "        [-0.1563, -0.1612, -0.1002,  0.2081,  0.0709,  0.0652,  0.0083,  0.1168,\n",
      "         -0.0195, -0.2082,  0.0878,  0.2262, -0.1879,  0.0866,  0.0485,  0.1718],\n",
      "        [ 0.1440, -0.1591,  0.2155, -0.1800,  0.0582,  0.1563, -0.0398,  0.2421,\n",
      "          0.1629,  0.1545,  0.1479, -0.2114,  0.2054, -0.1432,  0.2435,  0.1945],\n",
      "        [-0.2272, -0.0502, -0.0777,  0.1820, -0.0254, -0.0070,  0.1915,  0.0896,\n",
      "          0.0413, -0.1327,  0.0167,  0.1425,  0.1433,  0.0815,  0.0253,  0.1482],\n",
      "        [ 0.2102, -0.1233,  0.2472,  0.0674,  0.0725, -0.1547,  0.1175, -0.2441,\n",
      "          0.0144,  0.1502,  0.1363, -0.0139,  0.1431,  0.2376,  0.1442,  0.2325],\n",
      "        [-0.0115,  0.0621, -0.2148, -0.0107,  0.1859,  0.1497, -0.1483, -0.1158,\n",
      "         -0.1795, -0.1384,  0.1862,  0.2284,  0.0213,  0.1418,  0.0815,  0.1399],\n",
      "        [ 0.1396, -0.1229,  0.1227, -0.0591,  0.0143,  0.0575,  0.1474, -0.0523,\n",
      "          0.0693, -0.0655,  0.1011, -0.2143, -0.0063,  0.0829,  0.1087, -0.0648],\n",
      "        [-0.0782, -0.0903,  0.0143, -0.2435, -0.2171, -0.2182,  0.1520, -0.1236,\n",
      "         -0.1236,  0.2440, -0.0920,  0.1407,  0.1502,  0.2176, -0.0145,  0.0767],\n",
      "        [-0.2028, -0.0237,  0.0974,  0.0967, -0.0679,  0.1973, -0.0584,  0.0521,\n",
      "          0.2339,  0.0411, -0.0123, -0.1344, -0.0500,  0.1185,  0.1904,  0.2104],\n",
      "        [-0.0450, -0.2200, -0.2079, -0.2113,  0.0587, -0.1868,  0.1194,  0.1244,\n",
      "          0.1890, -0.0401, -0.0577, -0.2416,  0.2005, -0.0330, -0.0194, -0.0984],\n",
      "        [-0.0280,  0.1001, -0.0698, -0.1913,  0.1724, -0.1890,  0.0666, -0.0244,\n",
      "          0.1014, -0.1721, -0.0326,  0.2121, -0.1242, -0.0818, -0.2114,  0.2421],\n",
      "        [ 0.2215, -0.1207,  0.1402, -0.0334,  0.0980, -0.0998,  0.0184, -0.2209,\n",
      "         -0.1212, -0.2112,  0.0449,  0.0816, -0.1379,  0.2060, -0.0169, -0.0440],\n",
      "        [ 0.0003, -0.0839,  0.1185,  0.0573, -0.0449, -0.1332, -0.2312,  0.2411,\n",
      "          0.1449,  0.0502,  0.0604,  0.1045, -0.1325,  0.0715, -0.1694, -0.1435],\n",
      "        [ 0.2475,  0.1413,  0.0824,  0.1169, -0.0278, -0.1411,  0.0774,  0.0222,\n",
      "          0.2408,  0.2273,  0.1073, -0.0423, -0.0413,  0.1947,  0.1292,  0.1025],\n",
      "        [ 0.0804, -0.1633,  0.2299,  0.0124,  0.1630, -0.0844, -0.0248,  0.2301,\n",
      "          0.0744, -0.0135, -0.2201,  0.0896,  0.0322,  0.1611,  0.1469,  0.1476],\n",
      "        [-0.0354, -0.2202,  0.0562,  0.0016, -0.0590,  0.1258, -0.1935, -0.1811,\n",
      "          0.0781, -0.1697, -0.0413,  0.2373,  0.1906, -0.1564, -0.1216,  0.0281],\n",
      "        [-0.2356,  0.1997, -0.2310,  0.0650, -0.0553, -0.1807,  0.2297,  0.0212,\n",
      "         -0.1308, -0.0660,  0.1759,  0.2276, -0.0387, -0.2276, -0.0418,  0.1166],\n",
      "        [-0.0189,  0.0753,  0.0679,  0.2437, -0.0216, -0.0491,  0.1033,  0.1431,\n",
      "         -0.2052, -0.1355,  0.0610,  0.0080, -0.0157, -0.1616,  0.2300,  0.2229],\n",
      "        [ 0.1868, -0.1375, -0.2237, -0.0722,  0.1463,  0.2476, -0.2340,  0.0390,\n",
      "         -0.0876,  0.0798,  0.0838, -0.1413,  0.0763,  0.0372,  0.1730,  0.1735],\n",
      "        [-0.1585, -0.1278, -0.0745,  0.1988,  0.1961, -0.0293, -0.2039, -0.2175,\n",
      "         -0.1135,  0.0247, -0.0966,  0.1273, -0.0017,  0.1389,  0.2011,  0.2178],\n",
      "        [-0.1763, -0.0586,  0.1495, -0.1757, -0.2358,  0.0343,  0.1331,  0.1793,\n",
      "          0.1899,  0.1315,  0.1224,  0.0629,  0.0793,  0.0080, -0.0559, -0.1646],\n",
      "        [-0.1220, -0.0579,  0.0693, -0.0431, -0.0277, -0.1785,  0.1936,  0.0618,\n",
      "         -0.0005, -0.2498, -0.0342, -0.0728,  0.0084,  0.2185, -0.1177, -0.1875]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.1100, -0.0737, -0.0244, -0.2137,  0.1949, -0.0964, -0.0072, -0.1165,\n",
      "         0.2122,  0.2452, -0.1805, -0.0081,  0.1061,  0.0571, -0.1585,  0.1289,\n",
      "         0.1919,  0.2371, -0.1917, -0.0785,  0.1942, -0.0654, -0.1156, -0.2042,\n",
      "         0.0310, -0.0778,  0.0014,  0.1582, -0.2342,  0.0378,  0.0367,  0.0282],\n",
      "       requires_grad=True)]\n",
      "tensor([[-0.0483, -0.2413,  0.5762, -0.2351,  0.4718,  0.2349,  0.3756, -0.3499,\n",
      "          0.0748,  0.0946, -0.5612,  0.0550,  0.5357,  0.5721,  0.6744,  0.4075,\n",
      "          0.3711,  0.3650,  0.2555, -0.2818,  0.0210, -0.2702, -0.4639,  0.6956,\n",
      "          0.2077, -0.6590,  0.2448,  0.5540, -0.1450,  0.1594,  0.1312, -0.0578],\n",
      "        [ 0.0708,  0.0322,  0.3876, -0.3430,  0.3432,  0.0731,  0.3595, -0.2385,\n",
      "         -0.2029, -0.3482, -0.5968,  0.1698,  0.9045,  0.3046,  0.5485,  0.6051,\n",
      "          0.3579, -0.1007, -0.0490, -0.5850, -0.0642, -0.1508, -0.3578,  0.5574,\n",
      "          0.4160, -0.3590,  0.0702,  0.6943,  0.2182,  0.1227, -0.0211, -0.5330]],\n",
      "       grad_fn=<AddmmBackward0>) torch.Size([2, 32])\n"
     ]
    }
   ],
   "source": [
    "# We now build our first neural network layers and combine them into one model\n",
    "\n",
    "# First lets define an example Linear layer.\n",
    "# Initialize a Linear layer from PyTorch of dimension (in_features=16, out_features=32).\n",
    "\n",
    "# Code here\n",
    "\n",
    "linearLayer = nn.Linear(in_features=16, out_features=32)\n",
    "\n",
    "\n",
    "# Print the layer attributes and print the weight of the Linear layer\n",
    "# Forward a fitting random initialized 2d tensor through the layer and print the result\n",
    "# What is the shape of the passed (forwarded) random tensor?\n",
    "\n",
    "# Code here\n",
    "\n",
    "##attributes\n",
    "print(linearLayer.state_dict)\n",
    "##Weights\n",
    "print('weights', list(linearLayer.parameters()))\n",
    "##passs random 2d tensor through the linear layer\n",
    "result= linearLayer(torch.rand(size=(2,16)))\n",
    "\n",
    "print(result, result.shape)\n",
    "\n",
    "\n",
    "# Why does it work to just call an initialized layer by initialized_layer(input)?\n",
    "# Check the source code for the Linear Layer and its parent 'Module' class here: https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
    "# Explain in your own words why the forward function of the 'Linear' class is automatically called when passing an input through the layer, i. e. initialized_layer(input)\n",
    "# Hint: Check out the class inheritance!\n",
    "\n",
    "# Your explanation here\n",
    "\n",
    "# This works because in the Module class, which is the parent class to every layer defined in pytorch, \n",
    "# define the special python method __call__ which enables class instances to behave like functions and \n",
    "# can be called like a function. In this case it is defined to call de forward method\n",
    "\n",
    "\n",
    "# Build a sequential model with some linear layers stacked after each other. The number of layers is your choice, but be careful because it could cost a lot of time\n",
    "# to pass data through the sequential model afterwards. Start e. g. with three linear layers :)\n",
    "# You are not restricted to linear layers. Experiment a little bit here!\n",
    "\n",
    "# Code here\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 32),  # input layer (do not change the in_features size of this layer - we need it later)\n",
    "    # your layers\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(32, 32), \n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(32, 10), \n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(10,10),  # you can change the in_features of this layer but let the out_features at size 10 here - we need it layer\n",
    "    nn.Softmax(dim=-1)\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe9abe45660>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfGklEQVR4nO3df3DU953f8deukFaAVxtjkFYysqq6kOSQQy5AwIx/CE+tolw420p62M6k0EtcOwamnOx6QugcXDqDPOTMMVdiXLspMY2JybT+lTNnLB9IxMUkmOCaEEpxkINSUBRk0OoHrJD20z9UtieDwZ+vtXprV8/HzM6wP158P/rqi176srvvDTnnnAAAMBC2XgAAYOyihAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGBmnPUCPiyVSunkyZOKRqMKhULWywEAeHLOqaurS2VlZQqHr3yuM+pK6OTJkyovL7deBgDgE2ptbdXUqVOv+JhRV0LRaFSSdIu+pHHKN14NriTvuknemYGODzKwkuETnjDeP5RK+UfOJ/23M4LChRHvTKigwDszkOjyzuR9qsh/O2cT3hlJCo3z/xHp+vsDbSuX9OuC3tSO9M/zK8lYCT355JP63ve+p1OnTmnGjBnauHGjbr311qvmLv4X3Djla1yIEhrN8sL+P3RCo/x7Gg75f00KBSihAJmRFGQ/hAJl/I+HvBHazmAuQAnxNIL0/yaSfpynVDLywoTt27dr5cqVWr16tQ4ePKhbb71VtbW1OnHiRCY2BwDIUhkpoQ0bNugb3/iGvvnNb+qzn/2sNm7cqPLycm3evDkTmwMAZKlhL6G+vj4dOHBANTU1Q26vqanR3r17L3l8MplUIpEYcgEAjA3DXkKnT5/WwMCASkpKhtxeUlKitra2Sx7f0NCgWCyWvvDKOAAYOzL2ZtUPPyHlnLvsk1SrVq1SZ2dn+tLa2pqpJQEARplhf3Xc5MmTlZeXd8lZT3t7+yVnR5IUiUQUifi/HBQAkP2G/UyooKBAs2bNUmNj45DbGxsbNX/+/OHeHAAgi2XkfUL19fX6+te/rtmzZ+vmm2/W008/rRMnTuihhx7KxOYAAFkqIyW0ePFidXR06Lvf/a5OnTqlqqoq7dixQxUVFZnYHAAgS4Wcc856Ef9YIpFQLBZTte5iYgIkSaEAzxkGHX6bOn8+UG4khAsLvTOpvgvBNpYaCJYbAaH8ABMTCoL9LEn19ATKjXX97oKa9LI6OztVVHTlMUt8lAMAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzGZmijbEhHI16Z1Ld3d4Zl0z6Z7wTg0LjAvyTCAX4Xc6l/CP9/d6ZvGsmemckaSCR8N/Wtdf6b+fMGe+Mu9A3Ipmg8iZf550ZON2RgZVkB86EAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmmKKNwFJdXSOynSCTrUORSLCNOf/526lz50ZkO0EEmYYdeFsBJmIH+t4WFHhnUr293pmgxvJE7CA4EwIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGAaYILK+k2DvjOv0HaqbOn/ffTn+/d2ZEhfP8My7lv5kJE/y3o2D7LxQKeWeCfG9D48d7Z4IaFy/xzqS6uv0zPT3emVzBmRAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzDDBFYAO/bx+R7YQnTvQPORdoW66vzz+T8t9WKOw/7FOhfO/ISA7GDLbH/YUiBf6hrmDb6m/7fbAgPjbOhAAAZighAICZYS+htWvXKhQKDbnE4/Hh3gwAIAdk5DmhGTNm6I033khfz8sL8AFeAICcl5ESGjduHGc/AICryshzQseOHVNZWZkqKyt177336vjx4x/52GQyqUQiMeQCABgbhr2E5s6dq61bt2rnzp165pln1NbWpvnz56ujo+Oyj29oaFAsFktfysvLh3tJAIBRKuRcwDdUfEw9PT268cYb9dhjj6m+vv6S+5PJpJLJZPp6IpFQeXm5qnWXxgV4XwRyD+8TSoe8I+6C/9cz2uVNvs47M3D68r8EIzP63QU16WV1dnaqqKjoio/N+JtVJ06cqJtuuknHjh277P2RSESRSCTTywAAjEIZf59QMpnUkSNHVFpamulNAQCyzLCX0KOPPqrm5ma1tLTo5z//ub761a8qkUhoyZIlw70pAECWG/b/jvvd736n++67T6dPn9aUKVM0b9487du3TxUVFcO9KQBAlhv2Enr++eeH+69EDglPmOAfCvAig1Rvr/92Agp//o+8M+//6ae8MxPmnPbOnEsGGPYpafVNf++d+VrU/8n/I33+36e73vqWd+a6n07zzkhS0bZ93plwNOqdSXUFnLCaA5gdBwAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwEzGP9QOuSvIJ56menoysJJLXfjnswLlTvzrlHdm2/ynvTNfjPh/avCA819bXoBPY5WkE/3d3plT/f7b+WzBNd6ZI7dt8c78Zv4574wkrdzxJe/MwNnOQNsaqzgTAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYYYo2AhupidhtfzHfO/PTv1gfaFuTwwXemQkBMnvOe0e05jf3eGc++Lvr/Tck6brDSe9Mwc9+5Z1xSf/tnP/yF70z3WV53hlJmnz2rUA5fHycCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADDDAFMEF/YfCtm/4PPemdcCDCMtHXeNd0aSelN93pnKnz7gnbnx+QHvTMHuX3pn4vqtd0aSwoWF/qFxI/PjpPDvfuGfCbit8MSJ3plUb6//hpzzz+QIzoQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYYYApAgv98We8M69vfcY783/8Z33qQNJ/EKkk/dtHV3hnpv/3n3tnQvkF/pkAQ0VDBf7bkaSBRCJQzlfejE97Z46siHlnwr3Bft+etuqgf2gMDyMNgjMhAIAZSggAYMa7hPbs2aNFixaprKxMoVBIL7300pD7nXNau3atysrKNH78eFVXV+vw4cPDtV4AQA7xLqGenh7NnDlTmzZtuuz969ev14YNG7Rp0ybt379f8Xhcd955p7q6uj7xYgEAucX7hQm1tbWqra297H3OOW3cuFGrV69WXV2dJOnZZ59VSUmJtm3bpgcffPCTrRYAkFOG9TmhlpYWtbW1qaamJn1bJBLR7bffrr179142k0wmlUgkhlwAAGPDsJZQW1ubJKmkpGTI7SUlJen7PqyhoUGxWCx9KS8vH84lAQBGsYy8Oi4UCg257py75LaLVq1apc7OzvSltbU1E0sCAIxCw/pm1Xg8LmnwjKi0tDR9e3t7+yVnRxdFIhFFIpHhXAYAIEsM65lQZWWl4vG4Ghsb07f19fWpublZ8+fPH85NAQBygPeZUHd3t95777309ZaWFr3zzjuaNGmSbrjhBq1cuVLr1q3TtGnTNG3aNK1bt04TJkzQ/fffP6wLBwBkP+8Sevvtt7VgwYL09fr6eknSkiVL9MMf/lCPPfaYzp07p4cfflhnzpzR3Llz9frrrysajQ7fqgEAOSHk3OiatpdIJBSLxVStuzQulG+9HFxB3qf/mXfmx//wX70zsfB478zC//Un3hlJyvua/7TU/lOXf+VnVgvneUcu3PF578z3ntnsnZkV8R/K+lzXdd4ZSdr62X/iH0oFmLibY/rdBTXpZXV2dqqoqOiKj2V2HADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADAzLB+sirGloGj7139QR9y16/v8840Vb3knfnpp1/xzkjSp1c/7J3J6670zlz3K//h9RPa+70zF67xn4YtSb//l+e9Mw9/7h+8M0EmYgfxoz+rCRZMHfGOhPL9vyZ3oc87kys4EwIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGAaYILO/aa70z+f/BP/OLrRe8M4WhAe+MJB2v+0/eme6U/7DPX/Xle2fmFfoPI73ggu2H/FCwwae+fnOh2zvz58vqvTOF//MX3hlJyrtukndmoOODQNsaqzgTAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYBpghs4MwZ70zeL3q9M2v/xb3emc6/dd4ZSfrT6w95ZyaE+7wzsbwe78znCk56ZyaEC7wzktSb8v+agmzr7v/4mHdm6hu/9M6EpkzxzkjSwB/+4B8KhfwzLtjxmgs4EwIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGAaYILK+oyDszkEj4b+gPH3hHiuqS/tuRtKt3YoCUf2bc9TO8M08+M8E788bnfuSdkaRrwoXemXWnP+2dKfvrvd6ZlHdCUt+FIKlgxvAw0iA4EwIAmKGEAABmvEtoz549WrRokcrKyhQKhfTSSy8NuX/p0qUKhUJDLvPmzRuu9QIAcoh3CfX09GjmzJnatGnTRz5m4cKFOnXqVPqyY8eOT7RIAEBu8n5hQm1trWpra6/4mEgkong8HnhRAICxISPPCTU1Nam4uFjTp0/XAw88oPb29o98bDKZVCKRGHIBAIwNw15CtbW1eu6557Rr1y498cQT2r9/v+644w4lk5d/yWxDQ4NisVj6Ul5ePtxLAgCMUsP+PqHFixen/1xVVaXZs2eroqJCr776qurq6i55/KpVq1RfX5++nkgkKCIAGCMy/mbV0tJSVVRU6NixY5e9PxKJKBKJZHoZAIBRKOPvE+ro6FBra6tKS0szvSkAQJbxPhPq7u7We++9l77e0tKid955R5MmTdKkSZO0du1afeUrX1Fpaanef/99fec739HkyZN1zz33DOvCAQDZz7uE3n77bS1YsCB9/eLzOUuWLNHmzZt16NAhbd26VWfPnlVpaakWLFig7du3KxqNDt+qAQA5wbuEqqur5a4woG/nzp2faEHIHgPdPf6hcJ7/ds6c8d/MBP9hn5IUyi/wzrT85SzvzF/+2U+8M1+LdnhnJP9BpEH95+Zq78z0yDveGfcRr7S9knBhsOedU729/qFQyD8zhoeeMjsOAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGAm45+sihyWGhiRzQSZiB1o+rGk5JfmeGeOfmOzd+Y3F7q9M9I13omfdMcCbEf69//tfu/M9L/6pXcm0ETsiRO9MwqP3O/b4Wv8v0+prq4MrCQ7cCYEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADANMEVwo5B3JixV5Z9z5AEMuCwu9M5J0YqH/72WH+855Z2YU+A+5/B/nU96Zpx7+qndGkv7pz0ZmGGnelCnemYGOD7wzIzVsVxrbw0iD4EwIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGQaYIjjnvCMDZzszsJBL/eav5wXKHaz7G+/M6QH/4Zgn+ru9MyvW/zvvzJQ33vLOSJL/dzbY0FgXZNjnCA4jzbv2Wu/MwJkzGVhJ7uJMCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBkGmCKwUCTinXHJpHfmxJr53pn/Uvekd0aSYuHx3pnfD/R4ZxY97T+MtPypvd6ZvOsmeWckaaDjA+9M6vz5QNsazQbOnvXOhMb5/1h1/f3emVzBmRAAwAwlBAAw41VCDQ0NmjNnjqLRqIqLi3X33Xfr6NGjQx7jnNPatWtVVlam8ePHq7q6WocPHx7WRQMAcoNXCTU3N2vZsmXat2+fGhsb1d/fr5qaGvX0/P//E1+/fr02bNigTZs2af/+/YrH47rzzjvVFeTDqwAAOc3rGbTXXnttyPUtW7aouLhYBw4c0G233SbnnDZu3KjVq1errq5OkvTss8+qpKRE27Zt04MPPjh8KwcAZL1P9JxQZ+fgRzVPmjT4CpyWlha1tbWppqYm/ZhIJKLbb79de/de/pU9yWRSiURiyAUAMDYELiHnnOrr63XLLbeoqqpKktTW1iZJKikpGfLYkpKS9H0f1tDQoFgslr6Ul5cHXRIAIMsELqHly5fr3Xff1Y9//ONL7guFQkOuO+cuue2iVatWqbOzM31pbW0NuiQAQJYJ9GbVFStW6JVXXtGePXs0derU9O3xeFzS4BlRaWlp+vb29vZLzo4uikQiigR40yMAIPt5nQk557R8+XK98MIL2rVrlyorK4fcX1lZqXg8rsbGxvRtfX19am5u1vz5/u96BwDkNq8zoWXLlmnbtm16+eWXFY1G08/zxGIxjR8/XqFQSCtXrtS6des0bdo0TZs2TevWrdOECRN0//33Z+QLAABkL68S2rx5sySpurp6yO1btmzR0qVLJUmPPfaYzp07p4cfflhnzpzR3Llz9frrrysajQ7LggEAuSPknHPWi/jHEomEYrGYqnWXxoXyrZeDYZb8kznemZ88tdE7U5w30TsjSUf6er0z92141DtT8rf+w0hD+QXeGXehzzszuLHLv5DoSvKC/KKZH2DYZ98F70xqBN8sHy4s9M7k2vDXfndBTXpZnZ2dKioquuJjmR0HADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADAT6JNVgaBO1OR5Z2Jh/+nRJ/q7vTOS9K9+9efemfiTv/DOjNjo+gDTsAdz/r+fDiQSwbY1AsIj+FEyIzmxOxdwJgQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMA0wR2Ol/c7N35uBXNnhnOlMp78wvk3HvjCRN+vL/9s64sP9Q1ryiIu9MkAGh4cJC74wkpc6f986Exvn/OAlFIt6ZVE+PfybgUNHwhAmBcvj4OBMCAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghgGmCOye5bu9M7HweO/MX/3hj7wze+r9h6tK0jgd8A85/wGrQYaRBhFkEGlQrr9/RDIjKdXba72EnMeZEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADMMMEVgf3/Sf7DoyeSnvDNvNP6xd+bGfe96ZyTJfxTpyAlFIt4Zl0xmYCXA8OFMCABghhICAJjxKqGGhgbNmTNH0WhUxcXFuvvuu3X06NEhj1m6dKlCodCQy7x584Z10QCA3OBVQs3NzVq2bJn27dunxsZG9ff3q6amRj09PUMet3DhQp06dSp92bFjx7AuGgCQG7xemPDaa68Nub5lyxYVFxfrwIEDuu2229K3RyIRxePx4VkhACBnfaLnhDo7OyVJkyZNGnJ7U1OTiouLNX36dD3wwANqb2//yL8jmUwqkUgMuQAAxobAJeScU319vW655RZVVVWlb6+trdVzzz2nXbt26YknntD+/ft1xx13KPkRLxVtaGhQLBZLX8rLy4MuCQCQZQK/T2j58uV699139eabbw65ffHixek/V1VVafbs2aqoqNCrr76qurq6S/6eVatWqb6+Pn09kUhQRAAwRgQqoRUrVuiVV17Rnj17NHXq1Cs+trS0VBUVFTp27Nhl749EIooEeBMeACD7eZWQc04rVqzQiy++qKamJlVWVl4109HRodbWVpWWlgZeJAAgN3k9J7Rs2TL96Ec/0rZt2xSNRtXW1qa2tjadO3dOktTd3a1HH31Ub731lt5//301NTVp0aJFmjx5su65556MfAEAgOzldSa0efNmSVJ1dfWQ27ds2aKlS5cqLy9Phw4d0tatW3X27FmVlpZqwYIF2r59u6LR6LAtGgCQG7z/O+5Kxo8fr507d36iBQEAxg6maCOwaxYe9860TJzonansecs7E3gadjjPO5IXK/LODJw5451hIjZyEQNMAQBmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmGGAKQIL5Rf4hwYGvCPhwkLvzNUmvn+UUCjknQkyjBTAIM6EAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGBm1M2Ouzjzq18XpGDjvzBCQs5/zlqQTBABR8cpyOpS7kKwjQE5ql+D/yY+zgzHUVdCXV1dkqQ3tcN4JbiqID97+XkNjBldXV2KxWJXfEzIBR03nCGpVEonT55UNBq9ZKJxIpFQeXm5WltbVVRUZLRCe+yHQeyHQeyHQeyHQaNhPzjn1NXVpbKyMoXDV37WZ9SdCYXDYU2dOvWKjykqKhrTB9lF7IdB7IdB7IdB7IdB1vvhamdAF/HCBACAGUoIAGAmq0ooEolozZo1ikQi1ksxxX4YxH4YxH4YxH4YlG37YdS9MAEAMHZk1ZkQACC3UEIAADOUEADADCUEADCTVSX05JNPqrKyUoWFhZo1a5Z+9rOfWS9pRK1du1ahUGjIJR6PWy8r4/bs2aNFixaprKxMoVBIL7300pD7nXNau3atysrKNH78eFVXV+vw4cM2i82gq+2HpUuXXnJ8zJs3z2axGdLQ0KA5c+YoGo2quLhYd999t44ePTrkMWPhePg4+yFbjoesKaHt27dr5cqVWr16tQ4ePKhbb71VtbW1OnHihPXSRtSMGTN06tSp9OXQoUPWS8q4np4ezZw5U5s2bbrs/evXr9eGDRu0adMm7d+/X/F4XHfeeWd6DmGuuNp+kKSFCxcOOT527MitGYzNzc1atmyZ9u3bp8bGRvX396umpkY9PT3px4yF4+Hj7AcpS44HlyW++MUvuoceemjIbZ/5zGfct7/9baMVjbw1a9a4mTNnWi/DlCT34osvpq+nUikXj8fd448/nr7t/PnzLhaLuaeeespghSPjw/vBOeeWLFni7rrrLpP1WGlvb3eSXHNzs3Nu7B4PH94PzmXP8ZAVZ0J9fX06cOCAampqhtxeU1OjvXv3Gq3KxrFjx1RWVqbKykrde++9On78uPWSTLW0tKitrW3IsRGJRHT77bePuWNDkpqamlRcXKzp06frgQceUHt7u/WSMqqzs1OSNGnSJElj93j48H64KBuOh6woodOnT2tgYEAlJSVDbi8pKVFbW5vRqkbe3LlztXXrVu3cuVPPPPOM2traNH/+fHV0dFgvzczF7/9YPzYkqba2Vs8995x27dqlJ554Qvv379cdd9yhZDJpvbSMcM6pvr5et9xyi6qqqiSNzePhcvtByp7jYdRN0b6SD3+0g3PukttyWW1tbfrPN910k26++WbdeOONevbZZ1VfX2+4Mntj/diQpMWLF6f/XFVVpdmzZ6uiokKvvvqq6urqDFeWGcuXL9e7776rN99885L7xtLx8FH7IVuOh6w4E5o8ebLy8vIu+U2mvb39kt94xpKJEyfqpptu0rFjx6yXYubiqwM5Ni5VWlqqioqKnDw+VqxYoVdeeUW7d+8e8tEvY+14+Kj9cDmj9XjIihIqKCjQrFmz1NjYOOT2xsZGzZ8/32hV9pLJpI4cOaLS0lLrpZiprKxUPB4fcmz09fWpubl5TB8bktTR0aHW1tacOj6cc1q+fLleeOEF7dq1S5WVlUPuHyvHw9X2w+WM2uPB8EURXp5//nmXn5/vfvCDH7hf//rXbuXKlW7ixInu/ffft17aiHnkkUdcU1OTO378uNu3b5/78pe/7KLRaM7vg66uLnfw4EF38OBBJ8lt2LDBHTx40P32t791zjn3+OOPu1gs5l544QV36NAhd99997nS0lKXSCSMVz68rrQfurq63COPPOL27t3rWlpa3O7du93NN9/srr/++pzaD9/61rdcLBZzTU1N7tSpU+lLb29v+jFj4Xi42n7IpuMha0rIOee+//3vu4qKCldQUOC+8IUvDHk54liwePFiV1pa6vLz811ZWZmrq6tzhw8ftl5Wxu3evdtJuuSyZMkS59zgy3LXrFnj4vG4i0Qi7rbbbnOHDh2yXXQGXGk/9Pb2upqaGjdlyhSXn5/vbrjhBrdkyRJ34sQJ62UPq8t9/ZLcli1b0o8ZC8fD1fZDNh0PfJQDAMBMVjwnBADITZQQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMz8X2YIPwCe10VtAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We initialized our model in the previous section\n",
    "# Lets now also use the model to pass data through it\n",
    "\n",
    "# Use the following tensor and pass it through your model from above\n",
    "# You have to 'reformat' the tensor first\n",
    "\n",
    "# Code here\n",
    "\n",
    "data = torch.randn(size=(5, 1, 28, 28))\n",
    "data = data.squeeze().flatten(start_dim=1, end_dim=2)\n",
    "print(data.shape)\n",
    "result = model(data)\n",
    "\n",
    "# read the image 'mnist_9.jpg' from the downloaded folder with the 'torchvision' python package and pass it through the network\n",
    "# How does the tensor of the image looks like? Which information is in the different dimensions?\n",
    "\n",
    "# Code here\n",
    "##the image is in grayscale and the tensor contains the pixel intensiti which goes from 0 to 255 where 0 is white a 255 black\n",
    "image = torchvision.io.read_image('mnist_9.jpg').squeeze()\n",
    "\n",
    "\n",
    "\n",
    "forwardPass=model(image.flatten().type(torch.float32))\n",
    "\n",
    "# visualize the image from above with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Code here\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Neural Network Example Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   0%|          | 16/15000 [00:01<13:59, 17.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.3055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2784, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   0%|          | 43/15000 [00:01<04:53, 50.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.3109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   0%|          | 70/15000 [00:01<03:04, 81.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.2900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2728, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3164, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   1%|          | 97/15000 [00:01<02:29, 99.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.2380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3074, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   1%|          | 123/15000 [00:02<02:12, 111.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.2015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1398, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3478, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3297, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   1%|          | 151/15000 [00:02<02:02, 121.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.3792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.8652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.8156, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   1%|          | 182/15000 [00:02<01:49, 135.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.2842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2107, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4164, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   1%|▏         | 211/15000 [00:02<01:47, 137.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(1.9038, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1776, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   2%|▏         | 241/15000 [00:02<01:45, 140.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.0548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3432, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   2%|▏         | 271/15000 [00:03<01:43, 141.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.2255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1799, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.8628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3080, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2597, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   2%|▏         | 301/15000 [00:03<01:42, 142.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.0311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.5206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.7435, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1260, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9699, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9716, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4246, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   2%|▏         | 331/15000 [00:03<01:44, 140.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(2.0875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.4391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.5124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.8849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.3154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.2077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.1157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.9760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(1.8515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 10])\n",
      "tensor(2.0900, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1:   2%|▏         | 333/15000 [00:03<02:37, 92.98it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(1.5805, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 69\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m finished with loss: \u001b[39m\u001b[39m{\u001b[39;00mrunning_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m and accuracy \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39mtensor(running_accuracy)\u001b[39m.\u001b[39mmean()\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[39m# Run the model training with the name of your model variable, in this case 'model'\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m train_model(model\u001b[39m=\u001b[39;49mmodel, batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [4], line 53\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, batch_size, epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[1;32m     52\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 53\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     55\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n\u001b[1;32m     56\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m state_values:\n\u001b[1;32m    166\u001b[0m             s[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mfloat\u001b[39m(s[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m--> 168\u001b[0m \u001b[39m@_use_grad_for_differentiable\u001b[39m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, closure\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, grad_scaler\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    170\u001b[0m     \u001b[39m\"\"\"Performs a single optimization step.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m            supplied from ``grad_scaler.step(optimizer)``.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cuda_graph_capture_health_check()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is only the application of your defined model\n",
    "# You can use the following method to train your model and check its accuracy. You can also use parts of the code below for the following practicals.\n",
    "# Just execute this box and it uses the predefined model from the previous task to run a training procedure. The variable name of the model must be 'model' (or change it accordingly).\n",
    "# ATTENTION: No worries if you don't understand the implementation. This is just for showing you how your defined model performs in terms of accuracy.\n",
    "# We will discuss everything in this code in future practicals.\n",
    "\n",
    "# Refine your model multiple times and see how the different models perform in terms of accuracy.\n",
    "\n",
    "# We use the MNIST dataset to set the model\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader  , testloader\n",
    "\n",
    "\n",
    "def train_model(model, batch_size: int = 4, epochs: int = 10):\n",
    "    # we only consider the mnist train data for this example\n",
    "    train_loader, _ = load_mnist_data()\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = []\n",
    "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
    "            imgs, targets = imgs.to(device), targets.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(imgs.reshape(imgs.shape[0], -1)).to(device)\n",
    "            print(outputs.shape)\n",
    "            loss = criterion(outputs, targets)\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = torch.max(outputs, dim=1).indices\n",
    "            accuracy = (max_outputs.detach() == targets.detach()).to(dtype=torch.float32).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "    \n",
    "    \n",
    "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy {torch.tensor(running_accuracy).mean():.3f}')\n",
    "\n",
    "\n",
    "# Run the model training with the name of your model variable, in this case 'model'\n",
    "\n",
    "train_model(model=model, batch_size=4, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Individual testing just to see\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "index=10\n",
    "\n",
    "plt.imshow(testset.test_data[index])\n",
    "test=testset.test_data[index].to('cuda').flatten().type(torch.float32)\n",
    "print(testset.targets[index])\n",
    "result=model(test)\n",
    "print(result)\n",
    "print(torch.argmax(model(test)).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
