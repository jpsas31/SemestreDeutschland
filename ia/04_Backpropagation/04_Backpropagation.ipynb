{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Today is about implementing the backpropagation algorithm.\n",
    "We will use the Stochastic Gradient Descent optimizer for optimizing the weights of a custom neural network.\n",
    "\n",
    "You can use numpy or torch for creating tensors, but not for the backpropagation (e.g. loss.backward() )!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have to consider the following steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) What task do you want to solve?\n",
    "    - You could use the MNIST dataset (as used in the previous practicals\n",
    "2) How does the network structure looks like?\n",
    "    - Build your own neural network\n",
    "        - Keep the code structure as simple as possible and\n",
    "        - use the same style as PyTorch, e. g. initializing a linear layer by Linear(in_features, out_features)\n",
    "            - use appropriate initialized weights for the linear layer weight matrices\n",
    "        - for this task you can stick by Linear layers only, to not make your code to complex\n",
    "3) Choose an appropriate loss function for your task (e. g. cross entropy loss for classification tasks)\n",
    "4) Perform backpropagation with your network by using the SGD optimizer (you dont have to use batch sizes greater than 1, but you can :) )\n",
    "\n",
    "\n",
    "=> Code examples are provided for tasks 1, 2, 3 in an attached ipython notebook\n",
    "=> Only use them if you really need them, try to implement it by yourself first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to start now?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before you start, draw a simple neural network on a paper and do the backpropagation algorithm.\n",
    "E. g. using 3 layers with 2 neurons each and calculate the update of a weight in the very first layer.\n",
    "Use a simple loss function (which also has a simple to derivative)\n",
    "\n",
    "After the calculations, try to implement the backpropagation algorithm with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def load_mnist_data(root_path='./data', batch_size=4):\n",
    "    \"\"\"\n",
    "    Loads MNIST dataset into your directory.\n",
    "    You can change the root_path to point to a already existing path if you want to safe a little bit of memory :)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "class LinearLayer:\n",
    "    def __init__(self,input_size, output_size) -> None:\n",
    "        self.input_size= input_size\n",
    "        self.output_size= output_size\n",
    "        self.weights = torch.empty(self.input_size, self.output_size)\n",
    "        nn.init.xavier_normal_(self.weights)\n",
    "        self.bias = torch.nn.init.zeros_(torch.empty(( output_size)))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.currentZ = torch.matmul(x, self.weights) + self.bias\n",
    "        self.currentActivation = self.currentZ #linear activation funct\n",
    "        return self.currentActivation\n",
    "  \n",
    "    #the derivative of the identity activation function is 1\n",
    "    def derivative(self, x):\n",
    "        return torch.from_numpy(np.ones(x.shape)).type(torch.float32)\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self) -> None:\n",
    "        self.layers = [\n",
    "            LinearLayer(input_size=784, output_size=64),\n",
    "            LinearLayer(input_size=64, output_size=64),\n",
    "              LinearLayer(input_size=64, output_size=64),\n",
    "               LinearLayer(input_size=64, output_size=64),\n",
    "              LinearLayer(input_size=64, output_size=64),\n",
    "            LinearLayer(input_size=64, output_size=10)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        return self.forward(x)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.currentInput=x\n",
    "      \n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SGDoptim:\n",
    "    def __init__(self, model, lr) -> None:\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        \n",
    "\n",
    "    def step(self, deltas):\n",
    "        for layer, delta in enumerate(deltas):\n",
    "                \n",
    "            if(layer==0):\n",
    "                newWeights = torch.matmul(self.model.currentInput.T, delta)\n",
    "            else:\n",
    "                newWeights = torch.matmul(self.model.layers[layer-1].currentActivation.T, delta)\n",
    "            # print(self.model.layers[layer].bias.shape, delta.shape)\n",
    "            # print(self.model.currentInput.shape[1])\n",
    "            self.model.layers[layer].weights = self.model.layers[layer].weights - self.lr/self.model.currentInput.shape[0] * newWeights\n",
    "            self.model.layers[layer].bias = self.model.layers[layer].bias - self.lr/self.model.currentInput.shape[0] * torch.mean(delta, dim=0)\n",
    "            # print(self.model.layers[layer].bias.shape)\n",
    "            # print(torch.mean(delta, dim=0).shape)\n",
    "       \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your loss function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code \n",
    "\n",
    "import torch.nn.functional as F\n",
    "class crossEntropyLoss:\n",
    "    def __init__(self, model) -> None:\n",
    "        self.model = model\n",
    "    def __call__(self, logits, target):\n",
    "        \n",
    "        loss = []\n",
    "        y=[]\n",
    "\n",
    "        for targetInd,data in enumerate(logits):\n",
    "            currentTarget = np.zeros(data.shape[0])\n",
    "            currentTarget[target[targetInd]]=1\n",
    "            y.append(currentTarget)\n",
    "            currentErr = -1* torch.log(data.dot(torch.from_numpy(currentTarget).type(torch.float32)))\n",
    "            loss.append(currentErr)\n",
    "        y = torch.from_numpy( np.array(y) )\n",
    "        self.currentTarget = y\n",
    "        \n",
    "        # loss = np.array([F.cross_entropy(pred, t).item() for pred, t in zip(logits, target)])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, logits):\n",
    "        deltas=[]\n",
    "        deltas.append(self.derivative(logits, self.model.layers[-1]))\n",
    "        for layer in range(len(self.model.layers)-2,-1,-1):  \n",
    "            \n",
    "            deltas.insert(0, self.model.layers[layer+1].derivative(self.model.layers[layer].currentZ) *\n",
    "\n",
    "            (self.model.layers[layer+1].weights @ deltas[0].T).T)\n",
    "            \n",
    "    \n",
    "        return deltas\n",
    "\n",
    "    \n",
    "    def derivative(self, activation, layer):\n",
    "        # layer.derivative(layer.currentZ) * \n",
    "        \n",
    "        return (torch.sub(activation,self.currentTarget) ).type(torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Consider the following steps:\n",
    "# 1) Loop through your training data\n",
    "#   1. 1) Choose number of epochs (How often do you want to loop through your complete dataset?)\n",
    "# 2) Forward the data through your network\n",
    "# 3) Calculate the loss\n",
    "# 4) Perform backpropagation with SGD and update the weights\n",
    "#   4. 1) Choose a learning rate to update your weights\n",
    "# Repeat 1, 2, 3, 4 until the training converges or maximum epochs are reached\n",
    "\n",
    "import tqdm\n",
    "def train_model(model, batch_size: int = 4, epochs: int = 20):\n",
    "    # we only consider the mnist train data for this example\n",
    "    train_loader, _ = load_mnist_data(batch_size=batch_size)\n",
    "    criterion = crossEntropyLoss(model)\n",
    "    optimizer = SGDoptim(model, lr=0.001)\n",
    "    iterations = 0\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = []\n",
    "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
    "        \n",
    "            iterations += 1\n",
    "\n",
    "            outputs = model(imgs.reshape(imgs.shape[0], -1))\n",
    "          \n",
    "            outputs = F.softmax(outputs,dim=1)\n",
    "           \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            deltas = criterion.backward(outputs)\n",
    "            \n",
    "            optimizer.step(deltas)\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += np.mean(loss)\n",
    "\n",
    "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
    "            max_outputs = torch.max(outputs, dim=1).indices\n",
    "            accuracy = (max_outputs.detach() == targets.detach()).to(dtype=torch.float32).mean()\n",
    "            running_accuracy.append(accuracy)\n",
    "    \n",
    "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy {torch.tensor(running_accuracy).mean():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 1: 100%|██████████| 1875/1875 [00:08<00:00, 231.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished with loss: 0.789 and accuracy 0.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 2: 100%|██████████| 1875/1875 [00:06<00:00, 293.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished with loss: 0.416 and accuracy 0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 3: 100%|██████████| 1875/1875 [00:06<00:00, 291.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished with loss: 0.370 and accuracy 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 4: 100%|██████████| 1875/1875 [00:06<00:00, 306.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished with loss: 0.349 and accuracy 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 5: 100%|██████████| 1875/1875 [00:06<00:00, 295.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished with loss: 0.335 and accuracy 0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 6: 100%|██████████| 1875/1875 [00:06<00:00, 292.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished with loss: 0.325 and accuracy 0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 7: 100%|██████████| 1875/1875 [00:06<00:00, 309.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished with loss: 0.319 and accuracy 0.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 8: 100%|██████████| 1875/1875 [00:06<00:00, 308.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished with loss: 0.313 and accuracy 0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 9: 100%|██████████| 1875/1875 [00:06<00:00, 305.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished with loss: 0.308 and accuracy 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 10: 100%|██████████| 1875/1875 [00:06<00:00, 291.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished with loss: 0.304 and accuracy 0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 11: 100%|██████████| 1875/1875 [00:05<00:00, 317.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 finished with loss: 0.301 and accuracy 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 12: 100%|██████████| 1875/1875 [00:06<00:00, 308.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 finished with loss: 0.298 and accuracy 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 13: 100%|██████████| 1875/1875 [00:06<00:00, 297.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 finished with loss: 0.295 and accuracy 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 14: 100%|██████████| 1875/1875 [00:05<00:00, 312.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 finished with loss: 0.293 and accuracy 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 15: 100%|██████████| 1875/1875 [00:06<00:00, 288.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 finished with loss: 0.290 and accuracy 0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 16: 100%|██████████| 1875/1875 [00:06<00:00, 297.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 finished with loss: 0.289 and accuracy 0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 17: 100%|██████████| 1875/1875 [00:06<00:00, 302.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 finished with loss: 0.287 and accuracy 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 18: 100%|██████████| 1875/1875 [00:05<00:00, 313.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 finished with loss: 0.285 and accuracy 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 19: 100%|██████████| 1875/1875 [00:05<00:00, 323.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 finished with loss: 0.283 and accuracy 0.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iteration 20: 100%|██████████| 1875/1875 [00:06<00:00, 296.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 finished with loss: 0.282 and accuracy 0.920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_model(NeuralNet(), batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
